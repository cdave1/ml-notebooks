{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tutorial from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Why does the batch size matter? What is the importance of 64?\n",
    "# DataLoader \"wraps an iterable\" over the dataset, presumably for use in a loop, map, etc.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define our model\n",
    "# Inherits from nn.Module (so nn definitively stands for \"neural network\")\n",
    "# We're creating the layers and structure of the neural network here.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512), # why 512? I get why 28*28 is the feature input.\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10) # 10 classes.\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits # is the name \"logits\" significant here? yes: https://deepai.org/machine-learning-glossary-and-terms/logit\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) # lr = \"learning rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # primes the model for training. Can't tell what this does in the source other than set a flag to True\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute the prediction error of our model\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Propagate the loss back through the model's parameters with the optimizer\n",
    "        optimizer.zero_grad() # zero the gradients - not sure why we need to do this\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # set the model to \"evaluation\" mode. presumably this is different to \"training\" mode.\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------\n",
      "loss: 1.043253  [    0/60000]\n",
      "loss: 1.067827  [ 6400/60000]\n",
      "loss: 0.873667  [12800/60000]\n",
      "loss: 1.027742  [19200/60000]\n",
      "loss: 0.900610  [25600/60000]\n",
      "loss: 0.924967  [32000/60000]\n",
      "loss: 0.976301  [38400/60000]\n",
      "loss: 0.922535  [44800/60000]\n",
      "loss: 0.953156  [51200/60000]\n",
      "loss: 0.901692  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.910591 \n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 0.957439  [    0/60000]\n",
      "loss: 1.000268  [ 6400/60000]\n",
      "loss: 0.791617  [12800/60000]\n",
      "loss: 0.962625  [19200/60000]\n",
      "loss: 0.840908  [25600/60000]\n",
      "loss: 0.856907  [32000/60000]\n",
      "loss: 0.923343  [38400/60000]\n",
      "loss: 0.873875  [44800/60000]\n",
      "loss: 0.896018  [51200/60000]\n",
      "loss: 0.854331  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.858261 \n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 0.890890  [    0/60000]\n",
      "loss: 0.949567  [ 6400/60000]\n",
      "loss: 0.729614  [12800/60000]\n",
      "loss: 0.914470  [19200/60000]\n",
      "loss: 0.797841  [25600/60000]\n",
      "loss: 0.805576  [32000/60000]\n",
      "loss: 0.883109  [38400/60000]\n",
      "loss: 0.839789  [44800/60000]\n",
      "loss: 0.852853  [51200/60000]\n",
      "loss: 0.817995  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.818469 \n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 0.837201  [    0/60000]\n",
      "loss: 0.908803  [ 6400/60000]\n",
      "loss: 0.681093  [12800/60000]\n",
      "loss: 0.877520  [19200/60000]\n",
      "loss: 0.765019  [25600/60000]\n",
      "loss: 0.765940  [32000/60000]\n",
      "loss: 0.850449  [38400/60000]\n",
      "loss: 0.814638  [44800/60000]\n",
      "loss: 0.818958  [51200/60000]\n",
      "loss: 0.788746  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.786849 \n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 0.792659  [    0/60000]\n",
      "loss: 0.874265  [ 6400/60000]\n",
      "loss: 0.641890  [12800/60000]\n",
      "loss: 0.848220  [19200/60000]\n",
      "loss: 0.738684  [25600/60000]\n",
      "loss: 0.734602  [32000/60000]\n",
      "loss: 0.822539  [38400/60000]\n",
      "loss: 0.795021  [44800/60000]\n",
      "loss: 0.791349  [51200/60000]\n",
      "loss: 0.764079  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.760612 \n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 0.754626  [    0/60000]\n",
      "loss: 0.843767  [ 6400/60000]\n",
      "loss: 0.609257  [12800/60000]\n",
      "loss: 0.824095  [19200/60000]\n",
      "loss: 0.716546  [25600/60000]\n",
      "loss: 0.709248  [32000/60000]\n",
      "loss: 0.797872  [38400/60000]\n",
      "loss: 0.778683  [44800/60000]\n",
      "loss: 0.768191  [51200/60000]\n",
      "loss: 0.742634  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.738096 \n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 0.721543  [    0/60000]\n",
      "loss: 0.816470  [ 6400/60000]\n",
      "loss: 0.581565  [12800/60000]\n",
      "loss: 0.803557  [19200/60000]\n",
      "loss: 0.697426  [25600/60000]\n",
      "loss: 0.688212  [32000/60000]\n",
      "loss: 0.775317  [38400/60000]\n",
      "loss: 0.764546  [44800/60000]\n",
      "loss: 0.748257  [51200/60000]\n",
      "loss: 0.723600  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.718229 \n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 0.692482  [    0/60000]\n",
      "loss: 0.791763  [ 6400/60000]\n",
      "loss: 0.557624  [12800/60000]\n",
      "loss: 0.785567  [19200/60000]\n",
      "loss: 0.680578  [25600/60000]\n",
      "loss: 0.670442  [32000/60000]\n",
      "loss: 0.754446  [38400/60000]\n",
      "loss: 0.751992  [44800/60000]\n",
      "loss: 0.730793  [51200/60000]\n",
      "loss: 0.706371  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.700363 \n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 0.666685  [    0/60000]\n",
      "loss: 0.769148  [ 6400/60000]\n",
      "loss: 0.536718  [12800/60000]\n",
      "loss: 0.769525  [19200/60000]\n",
      "loss: 0.665521  [25600/60000]\n",
      "loss: 0.655244  [32000/60000]\n",
      "loss: 0.734995  [38400/60000]\n",
      "loss: 0.740641  [44800/60000]\n",
      "loss: 0.715427  [51200/60000]\n",
      "loss: 0.690637  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.684095 \n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 0.643578  [    0/60000]\n",
      "loss: 0.748495  [ 6400/60000]\n",
      "loss: 0.518233  [12800/60000]\n",
      "loss: 0.755022  [19200/60000]\n",
      "loss: 0.652100  [25600/60000]\n",
      "loss: 0.642007  [32000/60000]\n",
      "loss: 0.716816  [38400/60000]\n",
      "loss: 0.730282  [44800/60000]\n",
      "loss: 0.701832  [51200/60000]\n",
      "loss: 0.676110  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.669178 \n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "loss: 0.622840  [    0/60000]\n",
      "loss: 0.729640  [ 6400/60000]\n",
      "loss: 0.501694  [12800/60000]\n",
      "loss: 0.741681  [19200/60000]\n",
      "loss: 0.640186  [25600/60000]\n",
      "loss: 0.630342  [32000/60000]\n",
      "loss: 0.699840  [38400/60000]\n",
      "loss: 0.721049  [44800/60000]\n",
      "loss: 0.689793  [51200/60000]\n",
      "loss: 0.662686  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.655439 \n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "loss: 0.604128  [    0/60000]\n",
      "loss: 0.712313  [ 6400/60000]\n",
      "loss: 0.486789  [12800/60000]\n",
      "loss: 0.729386  [19200/60000]\n",
      "loss: 0.629578  [25600/60000]\n",
      "loss: 0.620058  [32000/60000]\n",
      "loss: 0.684060  [38400/60000]\n",
      "loss: 0.712932  [44800/60000]\n",
      "loss: 0.679346  [51200/60000]\n",
      "loss: 0.650151  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.642788 \n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "loss: 0.587159  [    0/60000]\n",
      "loss: 0.696483  [ 6400/60000]\n",
      "loss: 0.473392  [12800/60000]\n",
      "loss: 0.717983  [19200/60000]\n",
      "loss: 0.620165  [25600/60000]\n",
      "loss: 0.610995  [32000/60000]\n",
      "loss: 0.669429  [38400/60000]\n",
      "loss: 0.705791  [44800/60000]\n",
      "loss: 0.670205  [51200/60000]\n",
      "loss: 0.638344  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.631128 \n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "loss: 0.571750  [    0/60000]\n",
      "loss: 0.681918  [ 6400/60000]\n",
      "loss: 0.461229  [12800/60000]\n",
      "loss: 0.707342  [19200/60000]\n",
      "loss: 0.611710  [25600/60000]\n",
      "loss: 0.602935  [32000/60000]\n",
      "loss: 0.655927  [38400/60000]\n",
      "loss: 0.699589  [44800/60000]\n",
      "loss: 0.662311  [51200/60000]\n",
      "loss: 0.627199  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.620360 \n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "loss: 0.557661  [    0/60000]\n",
      "loss: 0.668532  [ 6400/60000]\n",
      "loss: 0.450110  [12800/60000]\n",
      "loss: 0.697446  [19200/60000]\n",
      "loss: 0.604036  [25600/60000]\n",
      "loss: 0.595732  [32000/60000]\n",
      "loss: 0.643464  [38400/60000]\n",
      "loss: 0.694373  [44800/60000]\n",
      "loss: 0.655553  [51200/60000]\n",
      "loss: 0.616685  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.610413 \n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "loss: 0.544665  [    0/60000]\n",
      "loss: 0.656196  [ 6400/60000]\n",
      "loss: 0.439918  [12800/60000]\n",
      "loss: 0.688273  [19200/60000]\n",
      "loss: 0.596959  [25600/60000]\n",
      "loss: 0.589264  [32000/60000]\n",
      "loss: 0.631991  [38400/60000]\n",
      "loss: 0.690058  [44800/60000]\n",
      "loss: 0.649775  [51200/60000]\n",
      "loss: 0.606692  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.601215 \n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "loss: 0.532617  [    0/60000]\n",
      "loss: 0.644848  [ 6400/60000]\n",
      "loss: 0.430535  [12800/60000]\n",
      "loss: 0.679648  [19200/60000]\n",
      "loss: 0.590391  [25600/60000]\n",
      "loss: 0.583415  [32000/60000]\n",
      "loss: 0.621418  [38400/60000]\n",
      "loss: 0.686503  [44800/60000]\n",
      "loss: 0.644852  [51200/60000]\n",
      "loss: 0.597093  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.592701 \n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "loss: 0.521393  [    0/60000]\n",
      "loss: 0.634297  [ 6400/60000]\n",
      "loss: 0.421818  [12800/60000]\n",
      "loss: 0.671439  [19200/60000]\n",
      "loss: 0.584124  [25600/60000]\n",
      "loss: 0.578074  [32000/60000]\n",
      "loss: 0.611647  [38400/60000]\n",
      "loss: 0.683652  [44800/60000]\n",
      "loss: 0.640720  [51200/60000]\n",
      "loss: 0.587840  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.584815 \n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "loss: 0.510911  [    0/60000]\n",
      "loss: 0.624429  [ 6400/60000]\n",
      "loss: 0.413724  [12800/60000]\n",
      "loss: 0.663691  [19200/60000]\n",
      "loss: 0.578227  [25600/60000]\n",
      "loss: 0.573042  [32000/60000]\n",
      "loss: 0.602692  [38400/60000]\n",
      "loss: 0.681439  [44800/60000]\n",
      "loss: 0.637268  [51200/60000]\n",
      "loss: 0.578851  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.577500 \n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "loss: 0.501034  [    0/60000]\n",
      "loss: 0.615161  [ 6400/60000]\n",
      "loss: 0.406185  [12800/60000]\n",
      "loss: 0.656334  [19200/60000]\n",
      "loss: 0.572572  [25600/60000]\n",
      "loss: 0.568230  [32000/60000]\n",
      "loss: 0.594463  [38400/60000]\n",
      "loss: 0.679789  [44800/60000]\n",
      "loss: 0.634313  [51200/60000]\n",
      "loss: 0.570159  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.570703 \n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for param in model.parameters():\n",
    "    print(type(param), param.size())\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    # X - the input data\n",
    "    # Y - the \n",
    "    X.to(device), y.to(device)\n",
    "    pred = model(X)\n",
    "    print(y.size())\n",
    "    print(pred.size())\n",
    "    print(pred)\n",
    "    print(y)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "torch.sum(x)\n",
    "\n",
    "#input = torch.randn(4, 4)\n",
    "input = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "])\n",
    "input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2975,  0.2710, -0.1188, -0.0707],\n",
       "        [-0.2548, -0.5435,  0.2937,  0.1601],\n",
       "        [-0.1119,  0.3462,  0.0803,  0.0285]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 1\n",
    "torch.manual_seed(random_seed) # without this, nn.Linear will produce different results each time.\n",
    "m = nn.Linear(3, 4, False)\n",
    "output = m(input)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
